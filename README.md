# End-to-End LLM Post-Pretraining Pipeline

A complete, production-style pipeline for LLM post-pretraining covering **Supervised Fine-Tuning (SFT)** and **GRPO Alignment**. This is a learning/portfolio project demonstrating the full workflow from data preprocessing to model alignment.

> **Note**: This is a learning project. The trained models are not production-grade but serve to demonstrate the complete pipeline.

---

## ğŸ”— Model Checkpoints

**HuggingFace**: *To be added*

---

## ğŸ—ï¸ Pipeline Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Base Model    â”‚â”€â”€â”€â”€â–¶â”‚   SFT Model     â”‚â”€â”€â”€â”€â–¶â”‚  Aligned Model  â”‚
â”‚  (StableLM 1.6B)â”‚     â”‚   (LoRA + SFT)  â”‚     â”‚    (GRPO)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
   Pre-trained LLM      Instruction-tuned       Safety-aligned
                         on UltraChat           with Reward Model
```

### Stage 1: Supervised Fine-Tuning (SFT)
- Fine-tune the base model on instruction-following data
- Uses **LoRA** (r=256) for parameter-efficient training
- Dataset: UltraChat 200K conversations

### Stage 2: GRPO Alignment
- Align the SFT model using Group Relative Policy Optimization
- Uses a **reward model** to score responses
- Dataset: PKU-SafeRLHF for safety alignment

---

## ğŸ› ï¸ Tech Stack

| Component | Details |
|-----------|---------|
| **Base Model** | `stabilityai/stablelm-2-1_6b` |
| **Reward Model** | `OpenAssistant/reward-model-deberta-v3-large` |
| **SFT Dataset** | `HuggingFaceH4/ultrachat_200k` |
| **Alignment Dataset** | `PKU-Alignment/PKU-SafeRLHF` |
| **Techniques** | LoRA, PEFT, TRL, GRPO |
| **Hardware** | NVIDIA A100 GPU |
| **Frameworks** | Transformers, TRL, PEFT, PyTorch |

---

## ğŸ“ Project Structure

```
end-to-end-post-pretraining/
â”œâ”€â”€ config/                 # Configuration classes
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py           # Model paths and checkpoints
â”‚   â”œâ”€â”€ lora.py            # LoRA hyperparameters
â”‚   â”œâ”€â”€ training.py        # Training arguments
â”‚   â””â”€â”€ data.py            # Dataset configs
â”œâ”€â”€ src/                   # Core source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py           # SFT training logic
â”‚   â”œâ”€â”€ align.py           # GRPO alignment logic
â”‚   â”œâ”€â”€ inference.py       # Model inference utilities
â”‚   â””â”€â”€ data_processing.py # Data loading and preprocessing
â”œâ”€â”€ scripts/               # CLI scripts
â”‚   â”œâ”€â”€ run_sft.py         # Launch SFT training
â”‚   â””â”€â”€ run_grpo.py        # Launch GRPO alignment
â”œâ”€â”€ docker/                # Containerization
â”‚   â””â”€â”€ Dockerfile         # Flask API + vLLM setup
â”œâ”€â”€ data/                  # Processed datasets (gitignored)
â”œâ”€â”€ ipynb/                 # Reference notebooks
â”‚   â”œâ”€â”€ finetune.py        # SFT training reference
â”‚   â””â”€â”€ alignment.py       # GRPO training reference
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env.example
```

---

## ğŸ“Š Sample Outputs

*Sample outputs will be added soon.*

| Prompt | Base Model | SFT Model | Aligned Model |
|--------|------------|-----------|---------------|
| *TBD* | *TBD* | *TBD* | *TBD* |

---

## ğŸš€ Quick Start

### Installation

```bash
git clone https://github.com/yourusername/end-to-end-post-pretraining.git
cd end-to-end-post-pretraining
pip install -r requirements.txt
```

### Configuration

Copy the example environment file and update paths:

```bash
cp .env.example .env
```

### Training

```bash
# Stage 1: Supervised Fine-Tuning
python scripts/run_sft.py

# Stage 2: GRPO Alignment
python scripts/run_grpo.py
```

### Inference

```python
from src.inference import generate_response

response = generate_response(
    prompt="What are some common misconceptions about AI?",
    model_type="aligned"  # or "base", "sft"
)
print(response)
```

---

## ğŸ“ License

This project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.
