"""
Supervised Fine-Tuning (SFT) training module.

This file exists as the entry point for SFT training. It will contain the core
training logic for fine-tuning the base model on instruction-following data
using LoRA for parameter-efficient training.

You will implement:
- Model loading and tokenizer setup
- LoRA adapter initialization
- Training loop with HuggingFace Trainer
- Checkpoint saving and logging
"""

# TODO: Implement SFT training logic
# Reference: ipynb/finetune.py

pass
